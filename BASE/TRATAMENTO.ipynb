{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "PASTA_ENTRADA = Path(r\"SISAWEB\\Mineracao\")  \n",
    "PADRAO_ARQS   = \"Mineracao tipo *.json\"\n",
    "PASTA_SAIDA   = Path(\"TRATADOS\")\n",
    "PASTA_SAIDA.mkdir(exist_ok=True)\n",
    "\n",
    "def to_number(v: Any) -> Union[int, float, Any]:\n",
    "    if not isinstance(v, str):\n",
    "        return v\n",
    "    s = v.strip().replace(\",\", \".\")\n",
    "    if re.fullmatch(r\"[+-]?(\\d+(\\.\\d+)?|\\.\\d+)\", s):\n",
    "        try:\n",
    "            f = float(s)\n",
    "            return int(f) if math.isfinite(f) and f.is_integer() else f\n",
    "        except Exception:\n",
    "            return v\n",
    "    return v\n",
    "\n",
    "def deep_convert_numbers(x: Any) -> Any:\n",
    "    if isinstance(x, dict):\n",
    "        return {k: deep_convert_numbers(v) for k, v in x.items()}\n",
    "    if isinstance(x, list):\n",
    "        return [deep_convert_numbers(i) for i in x]\n",
    "    return to_number(x)\n",
    "\n",
    "def is_info_error(raw: Any) -> bool:\n",
    "    if raw is None:\n",
    "        return True\n",
    "\n",
    "    if isinstance(raw, list):\n",
    "        return len(raw) == 0\n",
    "\n",
    "    if isinstance(raw, dict):\n",
    "        msg = (raw.get(\"message\") or raw.get(\"mensagem\") or \"\").strip().lower()\n",
    "        if \"nenhum registro encontrado\" in msg:\n",
    "            return True\n",
    "        if any(k in raw for k in (\"error\", \"erro\")):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    if isinstance(raw, str):\n",
    "        s = raw.strip()\n",
    "        if not s:\n",
    "            return True\n",
    "        low = s.lower()\n",
    "        if \"erro_http_\" in low or \"nenhum registro encontrado\" in low:\n",
    "            return True\n",
    "        try:\n",
    "            parsed = json.loads(s)\n",
    "            return is_info_error(parsed)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    return False\n",
    "\n",
    "def parse_informacao(raw: Any) -> List[Dict[str, Any]]:\n",
    "    if is_info_error(raw):\n",
    "        return []\n",
    "    if isinstance(raw, list):\n",
    "        return [deep_convert_numbers(o) if isinstance(o, dict) else o for o in raw]\n",
    "    if isinstance(raw, dict):\n",
    "        return [deep_convert_numbers(raw)]\n",
    "    if isinstance(raw, str):\n",
    "        s = raw.strip()\n",
    "        try:\n",
    "            parsed = json.loads(s)\n",
    "        except Exception:\n",
    "            try:\n",
    "                parsed = json.loads(s.encode(\"utf-8\").decode(\"unicode_escape\"))\n",
    "            except Exception:\n",
    "                return []\n",
    "        if isinstance(parsed, list):\n",
    "            return [deep_convert_numbers(o) if isinstance(o, dict) else o for o in parsed]\n",
    "        if isinstance(parsed, dict):\n",
    "            return [deep_convert_numbers(parsed)]\n",
    "        return [deep_convert_numbers(parsed)]\n",
    "    return []\n",
    "\n",
    "por_municipio: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "arquivos = sorted(PASTA_ENTRADA.glob(PADRAO_ARQS))\n",
    "if not arquivos:\n",
    "    raise SystemExit(f\"Nenhum arquivo encontrado em {PASTA_ENTRADA}/'{PADRAO_ARQS}'\")\n",
    "\n",
    "tipo_regex = re.compile(r\"tipo\\s*(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "for arq in arquivos:\n",
    "    m = tipo_regex.search(arq.stem)\n",
    "    if not m:\n",
    "        print(f\"Ignorando (n√£o consegui extrair tipo): {arq.name}\")\n",
    "        continue\n",
    "    tipo_val = int(m.group(1))\n",
    "    tipo_key = f\"{tipo_val:02d}\"\n",
    "\n",
    "\n",
    "    with open(arq, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Falha ao ler {arq.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        registros = data\n",
    "    elif isinstance(data, dict):\n",
    "        registros = data.get(\"data\") or data.get(\"registros\") or []\n",
    "        if not isinstance(registros, list):\n",
    "            registros = []\n",
    "    else:\n",
    "        try:\n",
    "            with open(arq, \"r\", encoding=\"utf-8\") as f2:\n",
    "                registros = [json.loads(ln) for ln in f2 if ln.strip()]\n",
    "        except Exception:\n",
    "            print(f\"Formato n√£o reconhecido: {arq.name}\")\n",
    "            continue\n",
    "\n",
    "    for rec in registros:\n",
    "        mun_id   = rec.get(\"Id\")\n",
    "        mun_nome = rec.get(\"Nome\")\n",
    "        data_str = rec.get(\"Data\")\n",
    "        info_raw = rec.get(\"Informacao\")\n",
    "\n",
    "        if mun_id is None or mun_nome is None or data_str is None:\n",
    "            continue\n",
    "\n",
    "        if is_info_error(info_raw):\n",
    "            continue\n",
    "\n",
    "        info_list = parse_informacao(info_raw)\n",
    "        if not info_list:\n",
    "            continue\n",
    "\n",
    "        if mun_id not in por_municipio:\n",
    "            por_municipio[mun_id] = {\n",
    "                \"sisaweb_id\": mun_id,\n",
    "                \"municipio_nome\": mun_nome,\n",
    "                \"datas\": {}\n",
    "            }\n",
    "        muni = por_municipio[mun_id]\n",
    "\n",
    "        if data_str not in muni[\"datas\"]:\n",
    "            muni[\"datas\"][data_str] = {}\n",
    "\n",
    "        if tipo_key not in muni[\"datas\"][data_str]:\n",
    "            muni[\"datas\"][data_str][tipo_key] = []\n",
    "\n",
    "        for info in info_list:\n",
    "            if isinstance(info, dict):\n",
    "                muni[\"datas\"][data_str][tipo_key].append(info)\n",
    "\n",
    "for muni in por_municipio.values():\n",
    "    datas_ord = {}\n",
    "    for dt in sorted(muni[\"datas\"].keys()):\n",
    "        tipos = muni[\"datas\"][dt]\n",
    "        tipos_ord = {k: tipos[k] for k in sorted(tipos.keys())}\n",
    "        datas_ord[dt] = tipos_ord\n",
    "    muni[\"datas\"] = datas_ord\n",
    "\n",
    "def sanitize(nome: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_-]+\", \"_\", nome).strip(\"_\")\n",
    "\n",
    "for mun_id, payload in por_municipio.items():\n",
    "    nome = sanitize(str(payload.get(\"municipio_nome\", mun_id)))\n",
    "    saida = PASTA_SAIDA / f\"{nome}.json\" \n",
    "    with open(saida, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Municipios gerados: {len(por_municipio)} | Pasta: {PASTA_SAIDA.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130db1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "PASTA_SAIDA = Path(\"TRATADOS\")  \n",
    "PASTA_TEMPO = Path(r\"NASA\\MINERACAO NASA\\data\")     \n",
    "EXT_TEMPO   = \"*.json\"\n",
    "\n",
    "\n",
    "def strip_accents_and_slug(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s or \"\")\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "municipios = {}     \n",
    "slug_to_muni_files = defaultdict(list)\n",
    "\n",
    "for arq in PASTA_SAIDA.glob(\"*.json\"):\n",
    "    try:\n",
    "        with open(arq, \"r\", encoding=\"utf-8\") as f:\n",
    "            payload = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao ler munic√≠pio {arq.name}: {e}\")\n",
    "        continue\n",
    "    nome = payload.get(\"municipio_nome\") or arq.stem\n",
    "    slug = strip_accents_and_slug(nome)\n",
    "    municipios[slug] = (arq, payload)\n",
    "    slug_to_muni_files[slug].append(arq)\n",
    "\n",
    "clima_files = list(PASTA_TEMPO.glob(EXT_TEMPO))\n",
    "clima_por_slug = defaultdict(list)\n",
    "for clima_arq in clima_files:\n",
    "    cidade_nome_arquivo = clima_arq.stem\n",
    "    slug = strip_accents_and_slug(cidade_nome_arquivo)\n",
    "    clima_por_slug[slug].append(clima_arq)\n",
    "\n",
    "erros = False\n",
    "\n",
    "dups_muni = {slug: files for slug, files in slug_to_muni_files.items() if len(files) > 1}\n",
    "if dups_muni:\n",
    "    erros = True\n",
    "    print(\"\\nDuplicidade de munic√≠pios (mesmo slug) em TRATADOS:\")\n",
    "    for slug, files in dups_muni.items():\n",
    "        print(f\"   - {slug}: \" + \", \".join(f.name for f in files))\n",
    "\n",
    "dups_clima = {slug: files for slug, files in clima_por_slug.items() if len(files) > 1}\n",
    "if dups_clima:\n",
    "    erros = True\n",
    "    print(\"\\nDuplicidade de cidades clim√°ticas (mesmo slug) em NASA:\")\n",
    "    for slug, files in dups_clima.items():\n",
    "        print(f\"   - {slug}: \" + \", \".join(f.name for f in files))\n",
    "\n",
    "if not dups_muni and not dups_clima:\n",
    "    slugs_muni = set(municipios.keys())\n",
    "    slugs_clima = set(clima_por_slug.keys())\n",
    "\n",
    "    muni_sem_clima = sorted(slugs_muni - slugs_clima)\n",
    "    clima_sem_muni = sorted(slugs_clima - slugs_muni)\n",
    "\n",
    "    if muni_sem_clima:\n",
    "        erros = True\n",
    "        print(\"\\nMunic√≠pios SEM arquivo clim√°tico correspondente (por slug):\")\n",
    "        for slug in muni_sem_clima:\n",
    "            muni_file = municipios[slug][0]\n",
    "            print(f\"   - {slug}  ‚Üê  {muni_file.name}\")\n",
    "\n",
    "    if clima_sem_muni:\n",
    "        erros = True\n",
    "        print(\"\\nArquivos clim√°ticos SEM munic√≠pio correspondente (por slug):\")\n",
    "        for slug in clima_sem_muni:\n",
    "            files = clima_por_slug[slug]\n",
    "            print(f\"   - {slug}  ‚Üê  \" + \", \".join(f.name for f in files))\n",
    "\n",
    "if erros:\n",
    "    print(\"\\nPr√©-flight falhou. Corrija os itens acima e rode novamente. Nenhum arquivo foi alterado.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "inseridos_total = 0\n",
    "\n",
    "for slug, (arq_muni, payload) in municipios.items():\n",
    "    clima_arq = clima_por_slug[slug][0]\n",
    "\n",
    "    try:\n",
    "        with open(clima_arq, \"r\", encoding=\"utf-8\") as f:\n",
    "            clima_lista = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao ler clima {clima_arq.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(clima_lista, list):\n",
    "        print(f\"Formato clim√°tico inesperado (n√£o √© lista): {clima_arq.name}\")\n",
    "        continue\n",
    "\n",
    "    datas = payload.setdefault(\"datas\", {})\n",
    "\n",
    "    clima_por_data = {}\n",
    "    for item in clima_lista:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        dt = item.get(\"date\")\n",
    "        if not dt:\n",
    "            continue\n",
    "        clima_por_data[dt] = {k: v for k, v in item.items() if k != \"date\"}\n",
    "\n",
    "    insercoes = 0\n",
    "\n",
    "    for dt, clima_dt in clima_por_data.items():\n",
    "        if dt not in datas:\n",
    "            datas[dt] = {}  \n",
    "\n",
    "        datas[dt][\"meteorologia\"] = clima_dt\n",
    "        insercoes += 1\n",
    "\n",
    "    if insercoes:\n",
    "        try:\n",
    "            payload[\"datas\"] = {k: datas[k] for k in sorted(datas)}\n",
    "            with open(arq_muni, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "            inseridos_total += insercoes\n",
    "        except Exception as e:\n",
    "            print(f\"Falha ao salvar {arq_muni.name}: {e}\")\n",
    "    else:\n",
    "        print(f\"{clima_arq.name}: nenhuma data v√°lida para inserir em {arq_muni.name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# === Caminhos ===\n",
    "PASTA_TRATADOS = Path(\"TRATADOS\")            \n",
    "ARQUIVO_GEOJS  = Path(r\"SUPORTE\\geojs-35-mun.json\")    \n",
    "\n",
    "def slugify_nome(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s or \"\")\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def extrair_ibge_id(feature: dict) -> str:\n",
    "    if isinstance(feature.get(\"id\"), (str, int)):\n",
    "        return str(feature[\"id\"])\n",
    "    props = feature.get(\"properties\") or {}\n",
    "    for k in (\"cod_muni\", \"CD_MUN\", \"CD_MUNIC\", \"CD_MUNICIP\", \"code\", \"id\"):\n",
    "        if k in props and props[k]:\n",
    "            return str(props[k])\n",
    "    return None\n",
    "\n",
    "tratados_by_slug = {}\n",
    "dups_tratados = defaultdict(list)\n",
    "\n",
    "for arq in sorted(PASTA_TRATADOS.glob(\"*.json\")):\n",
    "    try:\n",
    "        with open(arq, \"r\", encoding=\"utf-8\") as f:\n",
    "            payload = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao ler {arq.name}: {e}\")\n",
    "        continue\n",
    "    nome = payload.get(\"municipio_nome\") or arq.stem\n",
    "    slug = slugify_nome(nome)\n",
    "    tratados_by_slug[slug] = {\"file\": arq, \"payload\": payload, \"nome\": nome}\n",
    "    dups_tratados[slug].append(arq.name)\n",
    "\n",
    "with open(ARQUIVO_GEOJS, \"r\", encoding=\"utf-8\") as f:\n",
    "    geo = json.load(f)\n",
    "\n",
    "if not isinstance(geo, dict) or geo.get(\"type\") != \"FeatureCollection\":\n",
    "    raise SystemExit(\"GeoJSON inv√°lido: esperava FeatureCollection.\")\n",
    "\n",
    "features = geo.get(\"features\") or []\n",
    "if not isinstance(features, list):\n",
    "    raise SystemExit(\"GeoJSON inv√°lido: 'features' n√£o √© lista.\")\n",
    "\n",
    "feats_by_slug = defaultdict(list)\n",
    "feat_info_by_slug = {}\n",
    "\n",
    "for i, feat in enumerate(features):\n",
    "    props = feat.get(\"properties\") or {}\n",
    "    nome_geo = props.get(\"name\") or props.get(\"NAME\") or props.get(\"Name\") or \"\"\n",
    "    slug = slugify_nome(nome_geo)\n",
    "    feats_by_slug[slug].append(i)\n",
    "    feat_info_by_slug[slug] = {\n",
    "        \"feature\": feat,\n",
    "        \"geometry\": feat.get(\"geometry\"),\n",
    "        \"ibge_id\": extrair_ibge_id(feat),\n",
    "        \"nome_geo\": nome_geo,\n",
    "    }\n",
    "\n",
    "erros = False\n",
    "\n",
    "dups_t = {s: lst for s, lst in dups_tratados.items() if len(lst) > 1}\n",
    "if dups_t:\n",
    "    erros = True\n",
    "    print(\"\\nDuplicidade em TRATADOS (mesmo slug):\")\n",
    "    for s, lst in dups_t.items():\n",
    "        print(f\"   - {s}: {', '.join(lst)}\")\n",
    "\n",
    "dups_g = {s: idxs for s, idxs in feats_by_slug.items() if len(idxs) > 1}\n",
    "if dups_g:\n",
    "    erros = True\n",
    "    print(\"\\nDuplicidade no GeoJSON (mesmo slug):\")\n",
    "    for s, idxs in dups_g.items():\n",
    "        print(f\"   - {s}: features {idxs}\")\n",
    "\n",
    "slugs_tratados = set(tratados_by_slug.keys())\n",
    "slugs_geo      = set(feats_by_slug.keys())\n",
    "\n",
    "faltando_no_geo = sorted(slugs_tratados - slugs_geo)\n",
    "faltando_nos_tratados = sorted(slugs_geo - slugs_tratados)\n",
    "\n",
    "if faltando_no_geo:\n",
    "    erros = True\n",
    "    print(\"\\nEm TRATADOS mas N√ÉO no GeoJSON:\")\n",
    "    for s in faltando_no_geo:\n",
    "        print(f\"   - {s}  ‚Üê {tratados_by_slug[s]['file'].name}\")\n",
    "\n",
    "if faltando_nos_tratados:\n",
    "    erros = True\n",
    "    print(\"\\nNo GeoJSON mas N√ÉO em TRATADOS:\")\n",
    "    for s in faltando_nos_tratados:\n",
    "        ex = feat_info_by_slug.get(s, {}).get(\"nome_geo\", s)\n",
    "        print(f\"   - {s} (ex.: '{ex}')\")\n",
    "\n",
    "if erros:\n",
    "    print(\"\\nPr√©-flight falhou. Nada foi modificado.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "print(\"Pr√©-flight OK! Conjunto casa 1‚Äìpara‚Äì1. Inserindo em TRATADOS...\\n\")\n",
    "\n",
    "atualizados = 0\n",
    "\n",
    "for slug in sorted(slugs_tratados):\n",
    "    arq = tratados_by_slug[slug][\"file\"]\n",
    "    payload = tratados_by_slug[slug][\"payload\"]\n",
    "    info   = feat_info_by_slug[slug]\n",
    "\n",
    "    ibge_id = info[\"ibge_id\"]\n",
    "    geo_geom = info[\"geometry\"]\n",
    "\n",
    "    if geo_geom is None:\n",
    "        print(f\"Sem geometria para {slug} (pulando).\")\n",
    "        continue\n",
    "\n",
    "    payload.pop(\"geo_properties\", None)\n",
    "    payload.pop(\"geo_geometry\", None)\n",
    "    payload.pop(\"ibge_id\", None)\n",
    "\n",
    "    novo = OrderedDict()\n",
    "\n",
    "    if \"municipio_id\" in payload:\n",
    "        novo[\"municipio_id\"] = payload[\"municipio_id\"]\n",
    "        novo[\"ibge_id\"] = ibge_id\n",
    "        for k, v in payload.items():\n",
    "            if k in (\"municipio_id\", \"ibge_id\", \"geo_properties\", \"geo_geometry\"):\n",
    "                continue\n",
    "            novo[k] = v\n",
    "    else:\n",
    "        novo[\"ibge_id\"] = ibge_id\n",
    "        for k, v in payload.items():\n",
    "            if k in (\"ibge_id\", \"geo_properties\", \"geo_geometry\"):\n",
    "                continue\n",
    "            novo[k] = v\n",
    "\n",
    "    novo[\"geo_geometry\"] = geo_geom\n",
    "\n",
    "    try:\n",
    "        with open(arq, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(novo, f, ensure_ascii=False, indent=2)\n",
    "        atualizados += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao salvar {arq.name}: {e}\")\n",
    "\n",
    "print(f\"\\nResumo: {atualizados} munic√≠pios atualizados com 'geojs_id' e 'geo_geometry' (sem geo_properties).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "PASTA_TRATADOS = Path(\"TRATADOS\")        \n",
    "PASTA_SINAN    = Path(\"SINAN\")             \n",
    "PADRAO_CSV     = \"*.csv\"\n",
    "\n",
    "if not PASTA_TRATADOS.exists():\n",
    "    raise SystemExit(f\"Pasta TRATADOS n√£o encontrada em: {PASTA_TRATADOS.resolve()}\")\n",
    "if not PASTA_SINAN.exists():\n",
    "    raise SystemExit(f\"Pasta SINAN n√£o encontrada em: {PASTA_SINAN.resolve()}\")\n",
    "\n",
    "def agravo_from_filename(name: str) -> str:\n",
    "    u = name.upper()\n",
    "    if u.startswith(\"CHIK\"):\n",
    "        return \"chikungunya\"\n",
    "    if u.startswith(\"DENG\"):\n",
    "        return \"dengue\"\n",
    "    if u.startswith(\"ZIKA\"):\n",
    "        return \"zika\"\n",
    "    return \"desconhecido\"\n",
    "\n",
    "def norm_ibge_code(v) -> str | None:\n",
    "    if v is None or v == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        n = int(str(v).strip())\n",
    "        return f\"{n:07d}\"\n",
    "    except Exception:\n",
    "        s = re.sub(r\"\\D\", \"\", str(v))\n",
    "        return s.zfill(7) if s else None\n",
    "\n",
    "_dt_compact = re.compile(r\"^\\s*(\\d{4})[/-]?(\\d{2})[/-]?(\\d{2})\\s*$\")\n",
    "\n",
    "def norm_dt(s: str) -> str | None:\n",
    "    if s is None:\n",
    "        return None\n",
    "    m = _dt_compact.match(str(s))\n",
    "    if not m:\n",
    "        return None\n",
    "    y, mth, d = m.groups()\n",
    "    return f\"{y}-{mth}-{d}\"\n",
    "\n",
    "def sniff_delimiter(sample: str) -> str:\n",
    "    try:\n",
    "        return csv.Sniffer().sniff(sample, delimiters=\";,\\t\").delimiter\n",
    "    except Exception:\n",
    "        counts = {d: sample.count(d) for d in (\";\", \",\", \"\\t\")}\n",
    "        return max(counts, key=counts.get) if any(counts.values()) else \",\"\n",
    "\n",
    "def normalize_header_name(s: str) -> str:\n",
    "    s = (s or \"\").strip().upper()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = (\n",
    "        s.replace(\"√ç\", \"I\").replace(\"√â\", \"E\").replace(\"√ì\", \"O\")\n",
    "         .replace(\"√Ç\", \"A\").replace(\"√É\", \"A\").replace(\"√á\", \"C\")\n",
    "    )\n",
    "    return s\n",
    "\n",
    "def open_csv_detect_robust(path: Path):\n",
    "    with path.open(\"rb\") as fb:\n",
    "        raw_sample = fb.read(8192)\n",
    "\n",
    "    enc_candidates = [\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin-1\"]\n",
    "    sample_text = None\n",
    "    for enc in enc_candidates:\n",
    "        try:\n",
    "            sample_text = raw_sample.decode(enc, errors=\"strict\")\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if sample_text is None:\n",
    "        sample_text = raw_sample.decode(\"latin-1\", errors=\"replace\")\n",
    "\n",
    "    delim = sniff_delimiter(sample_text)\n",
    "\n",
    "    for enc in enc_candidates + [\"latin-1\"]:\n",
    "        f = None\n",
    "        try:\n",
    "            f = path.open(\"r\", encoding=enc, newline=\"\")\n",
    "            reader = csv.DictReader(f, delimiter=delim)\n",
    "            for _ in reader:\n",
    "                pass\n",
    "            f.close()\n",
    "            f = path.open(\"r\", encoding=enc, newline=\"\")\n",
    "            reader = csv.DictReader(f, delimiter=delim)\n",
    "            headers = reader.fieldnames or []\n",
    "            header_map = {normalize_header_name(h): h for h in headers}\n",
    "            return f, reader, header_map, delim, enc\n",
    "        except UnicodeDecodeError:\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    f.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            continue\n",
    "\n",
    "    f = path.open(\"r\", encoding=\"latin-1\", errors=\"replace\", newline=\"\")\n",
    "    reader = csv.DictReader(f, delimiter=delim)\n",
    "    headers = reader.fieldnames or []\n",
    "    header_map = {normalize_header_name(h): h for h in headers}\n",
    "    return f, reader, header_map, delim, \"latin-1(+replace)\"\n",
    "\n",
    "def find_col(header_map: dict, *candidates: str) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in header_map:\n",
    "            return header_map[c]\n",
    "    return None\n",
    "\n",
    "def slim_row(row: dict, srcfile: str, agravo: str, ibge: str, dt: str) -> dict:\n",
    "    d = {k: v for k, v in row.items() if v not in (\"\", None, \"\")}\n",
    "\n",
    "    d[\"_FONTE\"] = \"SINAN\"\n",
    "    d[\"_ARQUIVO\"] = srcfile\n",
    "    d[\"_AGRAVO\"] = agravo\n",
    "    d[\"_IBGE_ID\"] = ibge\n",
    "    d[\"_DATA\"] = dt\n",
    "    return d\n",
    "\n",
    "tratados_by_ibge: dict[str, tuple[Path, dict]] = {}\n",
    "dups_trat = defaultdict(list)\n",
    "\n",
    "for arq in sorted(PASTA_TRATADOS.glob(\"*.json\")):\n",
    "    try:\n",
    "        payload = json.loads(arq.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao ler {arq.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    ibge = norm_ibge_code(payload.get(\"ibge_id\"))  # usa ibge_id\n",
    "    if not ibge:\n",
    "        print(f\"{arq.name}: sem 'ibge_id' v√°lido ‚Äî ignorando.\")\n",
    "        continue\n",
    "\n",
    "    tratados_by_ibge[ibge] = (arq, payload)\n",
    "    dups_trat[ibge].append(arq.name)\n",
    "\n",
    "dups = {k: v for k, v in dups_trat.items() if len(v) > 1}\n",
    "if dups:\n",
    "    print(\"\\nDuplicidade de 'ibge_id' em TRATADOS:\")\n",
    "    for k, lst in dups.items():\n",
    "        print(f\"   - {k}: {', '.join(lst)}\")\n",
    "    raise SystemExit(\"\\nAbortado por duplicidades em TRATADOS.\")\n",
    "\n",
    "arquivos_csv = sorted(PASTA_SINAN.glob(PADRAO_CSV))\n",
    "if not arquivos_csv:\n",
    "    raise SystemExit(f\"Nenhum CSV encontrado em {PASTA_SINAN}/'{PADRAO_CSV}'\")\n",
    "\n",
    "ids_sinan = set()\n",
    "for csv_path in arquivos_csv:\n",
    "    fh, reader, header_map, delim, enc = open_csv_detect_robust(csv_path)\n",
    "    id_col = find_col(header_map, \"ID_MUNICIP\", \"ID_MUNICIPIO\")\n",
    "    dt_col = find_col(header_map, \"DT_NOTIFIC\")\n",
    "    if not id_col or not dt_col:\n",
    "        print(f\"\\nüîé DEBUG {csv_path.name}:\")\n",
    "        print(\"  encoding:\", enc, \"delimiter:\", repr(delim))\n",
    "        print(\"  colunas:\", reader.fieldnames)\n",
    "        fh.close()\n",
    "        raise SystemExit(f\"{csv_path.name}: n√£o encontrei colunas ID_MUNICIP/DT_NOTIFIC (veja DEBUG).\")\n",
    "\n",
    "    for row in reader:\n",
    "        ibge = norm_ibge_code(row.get(id_col))\n",
    "        if ibge:\n",
    "            ids_sinan.add(ibge)\n",
    "    fh.close()\n",
    "\n",
    "faltantes = sorted(ids_sinan - set(tratados_by_ibge.keys()))\n",
    "if faltantes:\n",
    "    print(\"\\nIDs do SINAN sem JSON correspondente (ibge_id) em TRATADOS:\")\n",
    "    for i in faltantes[:30]:\n",
    "        print(f\"   - {i}\")\n",
    "    if len(faltantes) > 30:\n",
    "        print(f\"   ... (+{len(faltantes)-30} outros)\")\n",
    "    raise SystemExit(\"\\nPr√©-flight falhou. Nada foi modificado.\")\n",
    "\n",
    "print(\"Pr√©-flight OK! Todos os ID_MUNICIP t√™m JSON correspondente via ibge_id.\\n\")\n",
    "\n",
    "\n",
    "acc: dict[str, dict[str, dict[str, list[dict]]]] = defaultdict(\n",
    "    lambda: defaultdict(lambda: defaultdict(list))\n",
    ")\n",
    "\n",
    "for csv_path in arquivos_csv:\n",
    "    agravo = agravo_from_filename(csv_path.stem)\n",
    "    print(f\"Lendo {csv_path.name} (agravo: {agravo}) ...\")\n",
    "\n",
    "    fh, reader, header_map, delim, enc = open_csv_detect_robust(csv_path)\n",
    "    id_col = find_col(header_map, \"ID_MUNICIP\", \"ID_MUNICIPIO\")\n",
    "    dt_col = find_col(header_map, \"DT_NOTIFIC\")\n",
    "    if not id_col or not dt_col:\n",
    "        print(f\"\\nüîé DEBUG {csv_path.name}:\")\n",
    "        print(\"  encoding:\", enc, \"delimiter:\", repr(delim))\n",
    "        print(\"  colunas:\", reader.fieldnames)\n",
    "        fh.close()\n",
    "        raise SystemExit(f\"{csv_path.name}: n√£o encontrei colunas ID_MUNICIP/DT_NOTIFIC.\")\n",
    "\n",
    "    dt_cols_norm = [c for c in header_map.keys() if c.startswith(\"DT\")]\n",
    "    dt_cols = [header_map[c] for c in dt_cols_norm]\n",
    "\n",
    "    for row in reader:\n",
    "        ibge = norm_ibge_code(row.get(id_col))\n",
    "        if not ibge:\n",
    "            continue\n",
    "\n",
    "        for c in dt_cols:\n",
    "            if c in row and row[c]:\n",
    "                row[c] = norm_dt(row[c]) or row[c]\n",
    "\n",
    "        dt = norm_dt(row.get(dt_col))\n",
    "        if not dt:\n",
    "            continue\n",
    "\n",
    "        acc[ibge][dt][agravo].append(\n",
    "            slim_row(dict(row), csv_path.name, agravo, ibge, dt)\n",
    "        )\n",
    "\n",
    "    fh.close()\n",
    "\n",
    "print(\"Agrega√ß√£o conclu√≠da.\\n\")\n",
    "\n",
    "salvos = 0\n",
    "for ibge, (src_path, base_payload) in tratados_by_ibge.items():\n",
    "    payload = json.loads(json.dumps(base_payload, ensure_ascii=False))\n",
    "    datas = payload.setdefault(\"datas\", {})\n",
    "\n",
    "    por_dia = acc.get(ibge, {})\n",
    "    for dt, por_agravo in por_dia.items():\n",
    "        if dt not in datas:\n",
    "            datas[dt] = {}\n",
    "        casos = datas[dt].setdefault(\"casos\", {})\n",
    "\n",
    "        for agravo, linhas in por_agravo.items():\n",
    "            atual = casos.get(agravo)\n",
    "            if isinstance(atual, list):\n",
    "                lst = atual\n",
    "            else:\n",
    "                lst = []\n",
    "                casos[agravo] = lst\n",
    "            lst.extend(linhas)\n",
    "\n",
    "    ordem = [\"municipio_id\", \"geojs_id\", \"ibge_id\", \"municipio_nome\", \"datas\", \"geo_geometry\"]\n",
    "    novo = OrderedDict()\n",
    "    for k in ordem:\n",
    "        if k in payload:\n",
    "            novo[k] = payload[k]\n",
    "    for k, v in payload.items():\n",
    "        if k not in novo:\n",
    "            novo[k] = v\n",
    "\n",
    "    with src_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(novo, f, ensure_ascii=False, indent=2)\n",
    "    salvos += 1\n",
    "\n",
    "print(f\"Pronto! {salvos} arquivos atualizados em: {PASTA_TRATADOS.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
